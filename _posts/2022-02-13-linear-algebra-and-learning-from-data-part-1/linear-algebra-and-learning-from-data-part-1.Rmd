---
title: "Linear Algebra and Learning from Data (PART 1)"
description: |
  Self-study notes from Gilbert Strang's recent book
author:
  - name: Ethan Tenison
    url: https://twitter.com/ethantenison
date: 02-13-2022
output:
  distill::distill_article:
    self_contained: false
    code_folding: yes
    toc: yes
    toc_float: yes
---

My route to becoming a data scientist has been unorthodox. I don't come from a math background, but I love to learn. Most of my journey into the field has been through self-study and completing independent projects. I started reading Gilbert Strang's recent book "Linear Algebra and Learning from Data" to increase my knowledge of the math used behind the scenes in machine learning algorithms. The book is dense, and since I'm training myself, I think the best way to solidify my learning is through writing up some blog posts in `Python` and `R`! 

Right now most of my work in machine learning is in Python, but R still has some of the best reporting tools, so I'm going to blend them both using the `reticulate` package.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(reticulate)

use_virtualenv("C:/Users/tenis/.virtualenvs/sunthetics_dash/")

```

# Highlights of Linear Algebra

The foundation for Linear Algebra is the equation $Ax=b$, where $A$ is a matrix, $x$ is a vector, and $b$ is the inner product, or dot product, of their matrix multiplication. Just for reference, inner products produce a scalar, and outer products produce a matrix. Many of the methods to reduce matrix dimensionality, such as SVD, are derivations of this formula. 

```{python}
import numpy as np

np.random.seed(27)

#creating a random matrix
A = np.random.randint(0,10,(3,2))
print(f"Matrix A\n", A)

x = np.random.randint(0,10,(2,1))
print(F"Vector x\n", x)

b = np.dot(A, x)
print(F"Vector b is the dot product of the x and A\n", b)


```

### Finding the basis 

One of the first things you learn about in linear algebra is about finding the basis elements. The basis of subspace is the set of independent vectors that can be combined to formulate the others. For example, in the case below, Row 3 is a combination of rows 1 and 2. To find the independent rows, or the rank, we can find the eigenvalues of the matrix. Eigenvalues that equal to zero translates to linear dependence. 

```{python}


import numpy as np

matrix = np.array(
    [
        [0, 1 ,0 ,0],
        [0, 0, 1, 0],
        [0, 1, 1, 0],
        [1, 0, 0, 1]
    ])

lambdas, V =  np.linalg.eig(matrix.T)
# The linearly dependent row vectors 
print(f'Linearly dependent rows',matrix[lambdas == 0,:])

```

### Eigenvectors and eigenvalues

Linear algebra helps us compress data. Eigenvectors and eigenvalues are what make this possible. $Ax=\lambda x$, where $A$ is a matrix, $x$ is the eigenvector, and $\lambda$ is the eigenvalue. Eigenvectors do not change when a transformation is applied to them, and the eigenvalue is the amount in which the orginal eigenvector is scaled to get matrix $A$. Generally speaking, you want Matrix $A$ to be a square matrix (m x m).  

<br/>
Let's test it out! 

```{python}
import scipy.linalg as la

A = np.array([[1,0],[0,-2]])
print(f"Square Matrix A\n", A)

eigvals, eigvecs = la.eig(A)
eigvals = eigvals.real

print(f"Eigenvalues x \n", eigvals)


print(f"Eigenvectors lambda \n", eigvecs)


```

### Singular Vectors and Singular Values

Singular vectors are similar to eigenvectors, in that they both describe the behavior of matrix. While eigenvectors describe the direction of a matrice's invariant^[**Invariant** signifies that something is unchanging. For example, counting is an invariant action because no matter where you stop the next value is always the same] action, singular vectors describe the direction of its maximum action. Since singular vectors describe the direction of maximum impact of a matrix, this becomes extremely important for dimension reduction. 

