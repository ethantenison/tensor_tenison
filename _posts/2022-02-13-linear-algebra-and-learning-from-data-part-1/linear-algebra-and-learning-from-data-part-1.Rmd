---
title: "Linear Algebra and Learning from Data (PART 1)"
description: |
  Self-study notes from Gilbert Strang's recent book
author:
  - name: Ethan Tenison
    url: https://twitter.com/ethantenison
date: 02-13-2022
output:
  distill::distill_article:
    self_contained: false
    code_folding: yes
    toc: yes
    toc_float: yes
---

My route to becoming a data scientist has been unorthodox. I don't come from a math background, but I love to learn. Most of my journey into the field has been through self-study and completing independent projects. I started reading Gilbert Strang's recent book "Linear Algebra and Learning from Data" to increase my knowledge of the math used behind the scenes in machine learning algorithms. The book is dense, and since I'm training myself, I think the best way to solidify my learning is through writing up some blog posts in `Python` and `R`! 

Right now most of my work in machine learning is in Python, but R still has some of the best reporting tools, so I'm going to blend them both using the `reticulate` package.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(reticulate)

use_virtualenv("C:/Users/tenis/.virtualenvs/sunthetics_dash/")

```

# Highlights of Linear Algebra

The foundation for Linear Algebra is the equation $Ax=b$, where $A$ is a matrix, $x$ is a vector, and $b$ is the inner product, or dot product, of their matrix multiplication. Just for reference, inner products produce a scalar, and outer products produce a matrix. 

```{python}
import numpy as np

#creating a random matrix
A = np.random.randint(0,10,(3,2))
print(f"Matrix A\n", A)

x = np.random.randint(0,10,(2,1))
print(F"Vector x\n", x)

b = np.dot(A, x)
print(F"Vector b is the dot product of the x and A\n", b)


```

### Finding the basis 

Finding the basis of subspace, or the set of independent vectors is essential for factoring. In the case below, Row 3 is a combination of  1 and 2, so it is linearly dependent. To find the independent rows, or the rank, we can find the eigenvalues of the matrix. Eigenvalues that equal to zero means that linear dependence. Therefore the basis, is much smaller than the original matrix.

```{python}


import numpy as np

matrix = np.array(
    [
        [0, 1 ,0 ,0],
        [0, 0, 1, 0],
        [0, 1, 1, 0],
        [1, 0, 0, 1]
    ])

lambdas, V =  np.linalg.eig(matrix.T)
# The linearly dependent row vectors 
print(f'Linearly dependent rows',matrix[lambdas == 0,:])

```

### Eigenvectors and eigenvalues

This ability to compress data is what makes eigenvectors and eigenvalues so powerful. This bringing us to the next important equation, $Ax=\lambda x$, where $A$ is a matrix, $x$ is the eigenvalue, and $\lambda$ is the eigenvector. Eigenvectors do not change when a transformation is applied to them, and the eigenvalue is the amount in which the orginal eigenvector is scaled to get matrix $A$.  