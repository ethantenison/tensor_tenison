---
title: "Linear Algebra and Learning from Data (PART 1)"
description: |
  Self-study notes from Gilbert Strang's recent book
author:
  - name: Ethan Tenison
    url: https://twitter.com/ethantenison
date: 02-13-2022
output:
  distill::distill_article:
    self_contained: false
    code_folding: yes
    toc: yes
    toc_float: yes
---

My route to becoming a data scientist has been unorthodox. I don't come from a math background, but I love to learn. Most of my journey into the field has been through self-study and completing independent projects. I started reading Gilbert Strang's recent book "Linear Algebra and Learning from Data" to increase my knowledge of the math used behind the scenes in machine learning algorithms. The book is dense, and since I'm training myself, I think the best way to solidify my learning is through writing up some blog posts in `Python` and `R`! 

Right now most of my work in machine learning is in Python, but R still has some of the best reporting tools, so I'm going to blend them both using the `reticulate` package.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(reticulate)

use_virtualenv("C:/Users/tenis/.virtualenvs/sunthetics_dash/")

```

# Highlights of Linear Algebra

The foundation for Linear Algebra is the equation $Ax=b$, where $A$ is a matrix, $x$ is a vector, and $b$ is the inner product, or dot product, of their matrix multiplication. Just for reference, inner products produce a scalar, and outer products produce a matrix. Many of the methods to reduce the dimensionality of a matrix, such as with SVD, are derivations of this formula. 

```{python}
import numpy as np

np.random.seed(27)

#creating a random matrix
A = np.random.randint(0,10,(3,2))
print(f"Matrix A\n", A)

x = np.random.randint(0,10,(2,1))
print(F"Vector x\n", x)

b = np.dot(A, x)
print(F"Vector b is the dot product of the x and A\n", b)


```

### Finding the basis 

One of the first things you learn about in linear algebra is about finding the basis elements. The basis of subspace is the set of independent vectors that can be combined to formulate the others. For example, in the case below, Row 3 is a combination of rows 1 and 2. To find the independent rows, or the rank, we can find the eigenvalues of the matrix. Eigenvalues that equal to zero translates to linear dependence. 

```{python}


import numpy as np

matrix = np.array(
    [
        [0, 1 ,0 ,0],
        [0, 0, 1, 0],
        [0, 1, 1, 0],
        [1, 0, 0, 1]
    ])

lambdas, V =  np.linalg.eig(matrix.T)
# The linearly dependent row vectors 
print(f'Linearly dependent rows',matrix[lambdas == 0,:])

```

### Eigenvectors and eigenvalues

Linear algebra, (so far that I've learned), is all about compressing data, and eigenvectors and eigenvalues are what make this possible. $Ax=\lambda x$, where $A$ is a matrix, $x$ is the eigenvalue, and $\lambda$ is the eigenvector. Eigenvectors do not change when a transformation is applied to them, and the eigenvalue is the amount in which the orginal eigenvector is scaled to get matrix $A$. Generally speaking, you want Matrix $A$ to be a square matrix (m x m).  

Let's test it out! 
```{python}
import scipy.linalg as la

A = np.random.randint(0,10,(3,3))
print(f"Square Matrix A\n", A)

eigvals, eigvecs = la.eig(A)

print(f"Eigenvalues x \n", eigvals)


print(f"Eigenvectors lambda \n", eigvecs)


```

To prove they work, lets convert $A$ to a symetric matrix $S$

```{python}


```



### Singular Vectors and eigenvalues 